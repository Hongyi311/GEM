{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "json_file_path = \"grounding_model_evals/gem7b_results/batch_0_to_9999\"   # Path to your JSON file\n",
    "\n",
    "json_file_save_dir = \"grounding_model_evals/gem7b_results/all_results_raw.json\"\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for result_file in os.listdir(json_file_path):\n",
    "    if result_file.endswith(\".json\"):\n",
    "        smp_id = result_file.split(\".\")[0]\n",
    "\n",
    "        with open(os.path.join(json_file_path, result_file), \"r\") as f:\n",
    "            result_data = json.load(f)\n",
    "\n",
    "        all_results[smp_id] = result_data[\"results\"]\n",
    "\n",
    "# Save the list of objects as a JSON array\n",
    "with open(json_file_save_dir, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "model_name = \"gem7b\"\n",
    "# file path\n",
    "data_dir = f\"grounding_model_evals/{model_name}_results/all_results_raw.json\"\n",
    "\n",
    "data = json.load(open(data_dir, \"r\"))\n",
    "\n",
    "expected_keys = [\n",
    "    'DiagnosisAccuracy',\n",
    "    'AnalysisCompleteness',\n",
    "    'AnalysisRelevance',\n",
    "    'LeadAssessmentCoverage',\n",
    "    'LeadAssessmentAccuracy',\n",
    "    'GroundedECGUnderstanding', # i.e., ECG Feature Grounding in paper\n",
    "    'EvidenceBasedReasoning', \n",
    "    'RealisticDiagnosticProcess' # i.e., Clinical Diagnostic Fidelity\n",
    "]\n",
    "\n",
    "pattern = re.compile(r'\\\"(?P<key>{})\\\":\\s*(?P<content>\\[.*?\\])'.format('|'.join(expected_keys)), re.DOTALL)\n",
    "\n",
    "result = {}\n",
    "\n",
    "# Additional cleaning functions\n",
    "def fix_unterminated_string(content):\n",
    "    quote_count = len(re.findall(r'(?<!\\\\)\"', content))\n",
    "    if quote_count % 2 == 1:\n",
    "        content = re.sub(r'(\\s*[}\\]])', r'\"\\1', content, count=1)\n",
    "    return content\n",
    "\n",
    "def escape_inner_quotes_in_explanation(content):\n",
    "    def replacer(match):\n",
    "        explanation = match.group(1)\n",
    "        fixed = re.sub(r'(?<!\\\\)\"', r'\\\\\"', explanation)\n",
    "        return f'\"Explanation\": \"{fixed}\"'\n",
    "    return re.sub(r'\"Explanation\":\\s*\"([^\"]*?)\"', replacer, content)\n",
    "\n",
    "def remove_extra_quotes(content):\n",
    "    content = re.sub(r'\"\"+', '\"', content)\n",
    "    return content\n",
    "\n",
    "def fix_unmatched_brackets(content):\n",
    "    def replacer(match):\n",
    "        explanation = match.group(1)\n",
    "        fixed = re.sub(r'[\\[\\]]', '', explanation)\n",
    "        return f'\"Explanation\": \"{fixed}\"'\n",
    "    return re.sub(r'\"Explanation\":\\s*\"([^\"]*?)\"', replacer, content)\n",
    "\n",
    "def fix_missing_commas(content):\n",
    "    content = re.sub(r'(\\})(\\s*\\{)', r'\\1,\\2', content)\n",
    "    return content\n",
    "\n",
    "def safe_eval(match):\n",
    "    try:\n",
    "        return str(eval(match.group(1)))\n",
    "    except:\n",
    "        return match.group(1)  # Return the original string if eval fails\n",
    "\n",
    "for id, content in data.items():\n",
    "    json_content = content.strip('```json\\n').strip('\\n```')\n",
    "    result[id] = {}\n",
    "\n",
    "    matches = pattern.finditer(json_content)\n",
    "    \n",
    "    for match in matches:\n",
    "        key = match.group('key')\n",
    "        content = match.group('content')\n",
    "\n",
    "        # Original cleaning steps\n",
    "        content = content.replace(\"\\\"\", '\"').replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        content = re.sub(r'//.*', '', content)\n",
    "        content = re.sub(r',\\s*([}\\]])', r'\\1', content)\n",
    "        content = re.sub(r'\"\\s*\"', ' ', content)\n",
    "        content = re.sub(r'\\+(\\d)', r'\\1', content)\n",
    "        content = re.sub(r'(\\d+[\\d\\s\\*\\+\\-\\/]+\\d+)', safe_eval, content)\n",
    "        content = content.replace('\");', '\"')\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        content = re.sub(r'\\n|\\r', ' ', content)\n",
    "\n",
    "        # Additional cleaning steps\n",
    "        content = fix_unterminated_string(content)\n",
    "        content = escape_inner_quotes_in_explanation(content)\n",
    "        content = remove_extra_quotes(content)\n",
    "        content = fix_unmatched_brackets(content)\n",
    "        content = fix_missing_commas(content)\n",
    "\n",
    "        open_braces = content.count('{')\n",
    "        close_braces = content.count('}')\n",
    "        if open_braces > close_braces:\n",
    "            content += '}' * (open_braces - close_braces)\n",
    "        \n",
    "        open_brackets = content.count('[')\n",
    "        close_brackets = content.count(']')\n",
    "        if open_brackets > close_brackets:\n",
    "            content += ']' * (open_brackets - close_brackets)\n",
    "\n",
    "        try:\n",
    "            content_json = json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decoding error for id {id}, key {key}: {e}\")\n",
    "            print(\"Content:\", content)\n",
    "            continue\n",
    "\n",
    "        scores = []\n",
    "        explanations = []\n",
    "\n",
    "        for item in content_json:\n",
    "            score = item.get('Score')\n",
    "            explanation = item.get('Explanation', '').strip()\n",
    "            extra_fields = {k: v for k, v in item.items() if k not in ['Score', 'Explanation']}\n",
    "\n",
    "            if extra_fields:\n",
    "                explanation += \" Additional details: \" + json.dumps(extra_fields)\n",
    "\n",
    "            scores.append(score)\n",
    "            explanations.append(explanation)\n",
    "\n",
    "        result[id][key] = {\n",
    "            'Scores': scores,\n",
    "            'Explanations': explanations\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of objects as a JSON array\n",
    "output_json_file = f\"grounding_model_evals/{model_name}_results/all_results_processed_clean.json\"\n",
    "with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"gem7b\"\n",
    "model_result_dir = f\"grounding_model_evals/{model_name}_results/all_results_processed_clean.json\"\n",
    "\n",
    "data = json.load(open(model_result_dir, \"r\"))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for id, content in data.items():\n",
    "    results[id] = {}\n",
    "    for key, value in content.items():\n",
    "        \n",
    "        if key in ['AnalysisCompleteness', 'AnalysisRelevance']:\n",
    "            # Filter out zero scores\n",
    "            average = sum(value['Scores'])\n",
    "        else:\n",
    "            # Filter out zero scores\n",
    "            filtered_lst = [x for x in value['Scores'] if x > 0] \n",
    "            average = sum(filtered_lst) / len(filtered_lst) if filtered_lst else 0\n",
    "\n",
    "        results[id][key] = average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for each diagnosis\n",
    "df['DiagnosisAccuracy'] = df['DiagnosisAccuracy']/2 * 100\n",
    "# remove outliers\n",
    "df['LeadAssessmentCoverage'] = df['LeadAssessmentCoverage'].clip(upper=12)/12 * 100\n",
    "# for each sample, the maximum score for LeadAssessmentAccuracy will be 24\n",
    "df['LeadAssessmentAccuracy'] = df['LeadAssessmentAccuracy']/24 * 100\n",
    "\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall scores\n",
    "(df['GroundedECGUnderstanding'].mean()+df['EvidenceBasedReasoning'].mean()+df['RealisticDiagnosticProcess'].mean())/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
